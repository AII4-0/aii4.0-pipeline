prepare:
  dataset: NASA-MSL
  batch_size: 256
  window_size: 100
train:
  model: Transformer
  epochs: 10
  num_encoder_layers: 3
  num_decoder_layers: 3
  dropout: 0.1
  prediction_length: 1
  learning_rate: 0.001